---
layout: post
title: ML| Classification Metrics
description: > 
  Classification Metrics에 대해 정리하였습니다.
categories: [ML]
tags: [Classification, Metrics]
---

'파이썬 머신러닝 완벽가이드(권철민 지음)'
{:.note title="출처"}

* this unordered seed list will be replaced by the toc
{:toc}

# confusion matrix
이진 분류에서 성능 지표로 잘 활용되는 오차행렬은 학습된 분류 모델이 예측을 수행하면서 얼마나 헷갈리고(confused) 있는지도 함께 보여주는 지표입니다.

![image](/assets/img/2024-11-13/confusion_matrix.png)

```python
from sklearn.metrics import confusion_matrix, accuracy_score, precision, recall, f1_score
```

- **TP(True Positive)**: 예측값을 Positive 값 1로 예측했고, 실제 값 역시 Positive 값 1
- **FP(False Positive)**: 예측값을 Positive 값 1로 예측했는데, 실제 값은 Negative 값 0
- **FN(False Negative)**: 예측값을 Negative 값 0으로 예측했는데, 실제 값은 Positive 값 1
- **TN(True Negative)**: 예측값을 Negative 값 0으로 예측했고, 실제 값 역시 Negative 값 0

## 정확도(accuracy)와 오분류율(error rate)
### 정확도(accuracy)
$$\frac{(TN+TP)}{(TN+FP+FN+TP)}$$

- 실제 데이터에서 예측 데이터가 얼마나 같은지 판단하는 지표입니다. 직관적으로 모델 예측 성능을 나타내는 평가 지표이며 이진 분류의 경우 데이터 구성에 따라 ML 모델의 성능을 왜곡할 수 있기 때문에 정확도 하나만 가지고 성능을 평가하지 않습니다.
- 특히나 불균형한 레이블 값 분포에서 ML 모델의 성능을 판단할 경우 적합한 평가 지표가 아닙니다.

### 오분류율(error rate)
$$\frac{(FN+FP)}{(TN+FP+FN+TP)} = 1 - accuracy$$
- 모형이 제대로 예측하지 못한 관측지를 평가하는 지표입니다.

## 정밀도(precision)와 재현율(recall)
정밀도와 재현율 지표 중에 이진 분류 모델의 업무 특성에 따라 특정 평가 지표가 더 중요한 지표로 간주됩니다.

- 정밀도: 상대적으로 더 중요한 지표인 경우에는 실제 Negative 음성인 데이터 예측을 Positive로 잘못 판단하게 되면 업무상 큰 영향이 발생하는 경우(스팸 메일)

- 재현율: 상대적으로 더 중요한 지표인 경우에는 실제 Positive 양성인 데이터 예측을 Negative로 잘못 판단하게 되면 업무상 큰 영향이 발생하는 경우(암 진단, 금융거래 사기 (주로 재현율이 정밀도보다 상대적으로 중요한 업부가 많음))

### 정밀도(precision)

$$\frac{TP}{(FP+TP)}$$

- 예측을 Positive로 한 대상 중에 예측과 실제 값이 Positive로 일치한 데이터의 비율을 뜻합니다.
- Positive 예측 성능을 더욱 정밀하게 측정하기 위한 평가 지표로 양성 예측도라고도 불립니다.

### 재현율(recall)

$$\frac{TP}{(FN+TP)}$$

- 실제 값이 Positive인 대상 중에 예측값과 실제 값이 Positive로 일치한 데이터의 비율을 뜻합니다.
- 민감도(Sensitivity) 또는 TPR(True Positive Rate)라고도 불립니다.

둘의 공식을 살펴보면 재현율과 정밀도 모두 TP를 높이는 데 동일하게 초점을 맞추지만, 재현율은 FN, 정밀도는 FP를 낮추는데 초점을 맞춥니다. 이를 통해 서로 보완적인 지표로서 성능을 평가하는 데 적용되는 것을 알 수 있습니다.

가장 좋은 성능 평가는 둘 다 높은 수치를 얻는 것입니다.

### 정밀도/재현율 트레이드 오프
분류하려는 업무의 특성상 정밀도 또는 재현율이 특별히 강조돼야 할 경우 임곗값(Threshold)을 조정해 수치를 높일 수 있습니다.

정밀도와 재현율은 상호 보안적인 평가 지표이기에 어느 한쪽을 강제로 높이면 다른 하나의 수치는 떨어지기 쉽습니다.

임계값을 낮추게 된다는 것은 모델이 Positive라고 예측하는 횟수가 많아진다는 것과 동일합니다. 그래서 상대적으로 재현율 값이 높아집니다. 양성 예측을 많이 하다보니 실제 양성을 음성으로 예측하는 횟수가 상대적으로 줄어들기 때문입니다.

Scikit-learn에서 임계치 변경은 Binarizer 클래스로 할 수 있습니다.

```python
from sklearn.preprocessing import Binarizer

class sklearn.preprocessing.Binarizer(*, threshold=0.0, copy=True)
```

class 생성시 threshold를 설정하며, fit_transform() 메서드를 통해 ndarray를 입력하면 threshold보다 같거나 작으면 0, 크면 1로 변환해 반환해줍니다.

```python
from sklearn.preprocessing import precision_recall_curve

sklearn.metrics.precision_recall_curve(y_true, y_score=None, *, pos_label=None, sample_weight=None, drop_intermediate=False, probas_pred='deprecated')
```

Scikit-learn의 precision_recall_curve를 통해 임곗값 별 정밀도와 재현율 값을 확인할 수 있습니다.

정밀도와 재현율을 각각 극단적으로 100%로 만들 수 있지만 수치가 적절하게 조합되어 분류의 종합적인 성능 평가에 사용될 수 있는 평가 지표가 필요합니다.

## F1 Score
정밀도와 재현율을 결합한 지표입니다. 정밀도와 재현율이 어느 한쪽으로 치우치지 않는 수치를 나타낼 때 상대적으로 높은 값을 가집니다.

$$\frac{2}{\frac{1}{recall}+\frac{1}{precision}} = 2 * \frac{precision*recall}{precision+recall}$$

## ROC Curve, AUC Score

### True Positive Rate(TPR, 재현율(민감도; sensitivity))
실제값 Positive(양성)가 정확히 예측돼야 하는 수준을 나타냅니다.(질병이 있는 사람은 질병이 있는 것으로 양성 판정)

$$TPR = \frac{TP}{FN+TP}$$

### True Negative Rate(TNR, 특이도; specificity)
실제값 Negative(음성)가 정확히 예측돼야 하는 수준을 나타냅니다.(질병이 없는 건강한 사람은 질병이 없는 것으로 음성 판정)

$$TNR = \frac{TN}{TN+FP}=\frac{TN}{N}$$

### False Positive Rate(FPR)
실제 Negative(음성)인 샘플 중에서 Positive(양성)으로 잘못 예측한 비율을 나타냅니다.

$$FPR = \frac{FP}{FP+TN} = 1 - TNR$$

### False Negative Rate(FNR)
실제 Positive(양성)인 샘플 중에서 Negative(음성)으로 잘못 예측한 비율을 나타냅니다.

$$FNR = \frac{FN}{TP+FN} = 1 - TPR$$

### ROC Curve, AUC Score
```python
from sklearn.metrics import roc_curve
sklearn.metrics.roc_curve(y_true, y_score, *, pos_label=None, sample_weight=None, drop_intermediate=True)
```

![image](/assets/img/2024-11-13/roc_curve.png)

**x축이 FPR, y축이 TPR**인 그래프로 둘의 관계를 표현합니다. 데이터가 균형인 경우 사용합니다. 왜냐하면 TPR과 FPR은 데이터 불균형에 영향을 받지 않는 지표이기 때문입니다.

ROC Curve는 **AUC(Area Under Curve: 그래프 아래의 면적)**을 이용해 모델의 성능을 평가합니다.

AUC가 1에 가까울수록 정확히 분류함을 의미합니다.

## PR Curve
**x축을 recall, y축을 precision**으로 두고 시각화한 그래프입니다. 데이터가 불균형한 상황에서 더 유용한 경우가 많습니다. 왜냐하면 precision은 **데이터 불균형**에 영향을 받는 지표이기 때문입니다.

ROC Curve와 마찬가지로 AUC로 평가합니다.