---
layout: post
title: ANN, DNN, CNN(Convolution, Padding, Stride, Pooling), RNN
description: > 
  ANN, DNN, CNN, RNN 정리
categories: [AI]
tags: [ANN, DNN, CNN, Convolution, Padding, Stride, Pooling]
---

{:.note title="출처 및 참고"}

* this unordered seed list will be replaced by the toc
{:toc}


# ANN(Arificial Neural Network)

![image](/assets/img/2025/ANN.jpeg)

인공신경망: **사람의 신경망 원리와 구조를 모방하여 만든 기계학습 알고리즘**

- 다수의 입력 데이터를 받는 입력층, 데이터의 출력을 담당하는 출력층, 은닉층이 존재하고 히든 레이어들의 갯수와 노드의 개수를 구성하는 것을 모델을 구성한다고 함
- 이 모델을 잘 구성하여 원하는 출력 값을 잘 예측하는 것이 우리가 해야할 일이고 은닉층에서는 활성화함수를 사용하여 최적의 Weight와 Bias를 찾아내는 역할을 함

## 문제점
1. 학습과정에서 파라미터의 최적값을 찾기 어려움(출력값을 결정하는 활성화 함수의 사용은 기울기 값에 의해 weight가 결정되었는데 이런 gradient 값이 뒤로 갈 수록 점점 작아져 0에 수렴하는 오류를 낳기도 하고 부분적인 에러를 최저 에러로 인식하여 더이상 학습을 하지 않는 경우도 있음)
2. Overfitting에 따른 문제
3. 학습시간이 너무 느림

-> 점점 해결 중, 느린 것은 그래픽 발전, 오버피팅은 사전 훈련

# DNN(Depp Neural Network)
![image](/assets/img/2025/DNN.png)

ANN기법의 여러 문제가 해결되면서 **모델 내 은닉층을 많이 늘려서 학습의 결과를 향상시키는 방법**으로 **은닉층을 2개이상** 지닌 학습 방법

**컴퓨터가 스스로 분류 레이블을 만들어 내고 공간을 왜곡하고 데이터를 구분짓는 과정을 반복하여 최적의 구번선을 도출**해냄, 많은 데이터와 반복학습, 사전학습과 오류역전파 기법을 통해 현재 널리 사용되고 있음

**DNN을 응용한 것이 CNN, RNN이고 LSTM, GRU가 있음**

## 한계점
- 은닉층이 깊어지면서 입력층으로 갈수록 gradient vanishing(기울기 소실) 문제가 발생할 수 있고, overfitting(과적합) 문제가 발생할 수 있음
- 해결 방안: CNN 등등

# CNN(합성곱신경망: Convolution Neural Network)
![image](/assets/img/2025/CNN.png)

기존의 방식은 데이터에서 지식을 추출해 학습이 이뤄짐, CNN은 데이터의 특징을 추출하여 특징들의 패턴을 파악하는 구조

DNN에서 발생한 overfitting(과적합) 문제에 해결책 중 하나로 나온 것이 CNN

**필터를 이용한 Convolution과 Pooling 연산을 반복적으로 진행하면서 이미지의 특징을 검출**

## Convolution(합성곱 연산, 필터 연산)
- 이미지 전처리에서 말하는 필터 연산

![image](/assets/img/2025/CNN3.png)

해당 결과에 편향까지 더해서
```text
18  19 
9   18
```

![image](/assets/img/2025/CNN2.png)
- **데이터의 특징을 추출하는 과정**
- 데이터에 각 성분의 인접 성분들을 조사해 특징을 파악하고 파악한 특징을 한장으로 도출시키는 과정
- 도출된 장을 Convolution Layer라고 함, 이 과정은 하나의 압축 과정이며 파라미터의 갯수를 효고적으로 줄여주는 역할을 함

### Padding
![image](/assets/img/2025/CNN4.jpeg)

합성곱을 수행하기 전에 입력 데이터 주변을 특정 값 (보통 0)으로 채움

> 주로 출력 크기를 조정할 목적으로 사용 (4, 4) 입력 데이터에 (3, 3) 필터를 적용하면 출력은 (2, 2)가 되서 입력보다 2만큼 줄어 듦, 이는 합성곱 연산을 몇 번이나 되풀이 하는 심층 신경망에서는 문제가 됨, 합성곱 연산을 거칠 때마다 크기가 작아지면 어느 시점에서는 1이 되고 더이상 합성곱이 불가능해서 풀링을 적용

### Stride
![image](/assets/img/2025/CNN5.png)

필터를 적용하는 위치의 간격
크기가 (7, 7)인 입력 데이터에서 스트라이드를 2로 설정한 필터를 적용

스트라이드를 2로하면 (3, 3)이 출력

스트라이드를 키우면 출력의크기는 작아짐!

반면, 패딩하면 출력 크기는 커짐

### 예시
- 입력 크기: (H, W)
- 필터 크기: (FH, FW)
- 패딩: P
- 스트라이드: S

출력 크기

$$
OH = \frac{H+2P-FH}{S} + 1 \\
OW = \frac{W+2P-FW}{S} + 1
$$

- 정수가 나누어 떨어져야 함

## Pooling
- **Convolution 과정을 거친 레이어의 사이즈를 줄여주는 과정**으로 단순히 데이터의 사이즈를 줄여주고, 노이즈를 상쇄시키고 미세한 부분에서 일관적인 특징을 제공

![image](/assets/img/2025/CNN6.png)
- Max Pooling을 스트라이드 2로 처리하는 순서

- 최대 풀링: 대상 영역에서 최댓값을 취하는 연산
- 평균 풀링: 대상 영역에서 평균값을 취하는 연산
- 이미지 인식에서는 주로 **최대 풀링**

### 특징
1. 학습해야할 매개변수가 없음
2. 채널 수가 변하지 않음
3. 입력의 변화에 영향을 적게 받음(강건함)

## 합성곱/풀링 계층 구현
### 합성곱 계층 구현
```python
class Convolution:
  def __init__(self, w, b, stride=1, pad=0):
    self.W = W
    self.b = b
    self.stride = stride
    self.pad = pad
  
  def forward(self, x):
    FN, C, FH, FW = self.W.shape
    N, C, H, W = x.shape
    out_h = int(1 + (H + 2 * self.pad - FH) / self.stride)
    out_w = int(1 + (W + 2 * self.pad - FW) / self.stride)

    col = im2col(x, FH, FW, self.stride, self.pad) # 필터 크기, 스트라이드, 패딩을 고려해서 입력 데이터를 2차원 배열로 전개한 함수
    col_w = self.W.reshape(FN, -1).T # 필터도 2차원 배열로 전개
    out = np.dot(col, col_W) + self.b # 행렬 곱

    out = out.reshpae(N, out_h, out_w, -1).transpose(0, 3, 1, 2) # 출력 데이터를 적절한 형상으로 변경
    return out
```
transpose: 인덱서(번호)로 축의 순서를 변경

### 풀링 계층 구현
```python
class Pooling:
  def __init__(self, pool_h, pool_w, stride=1, pad=0):
    self.pool_h = pool_h
    self.pool_w = pool_w
    self.stride = stride
    self.pad = pad

  def forward(self. w):
    N, C, H, W = x.shape
    out_h = int(1 + (H - self.pool_h) / self.stride)
    out_w = int(1 + (W - self.pool_w) / self.stride)

    col = im2(col(x, self.pool_h, self.pool_w, self.stride, self.pad))
    col = col.reshape(-1, self.pool_h * self.pool_w)

    out = np.max(col, axis=1)

    out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)

    return out
```

1. 입력 데이터를 전개
2. 행별 최댓값 구함
3. 적절한 모양으로 성형

## CNN 구현
```python
# coding: utf-8
import sys, os
sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정
import pickle
import numpy as np
from collections import OrderedDict
from common.layers import *
from common.gradient import numerical_gradient


class SimpleConvNet:
    """단순한 합성곱 신경망
    
    conv - relu - pool - affine - relu - affine - softmax
    
    Parameters
    ----------
    input_size : 입력 크기（MNIST의 경우엔 784）
    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）
    output_size : 출력 크기（MNIST의 경우엔 10）
    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'
    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）
        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정
        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정
    """
    def __init__(self, input_dim=(1, 28, 28), 
                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},
                 hidden_size=100, output_size=10, weight_init_std=0.01):
        filter_num = conv_param['filter_num']
        filter_size = conv_param['filter_size']
        filter_pad = conv_param['pad']
        filter_stride = conv_param['stride']
        input_size = input_dim[1]
        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1
        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))

        # 가중치 초기화
        self.params = {}
        self.params['W1'] = weight_init_std * \
                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)
        self.params['b1'] = np.zeros(filter_num)
        self.params['W2'] = weight_init_std * \
                            np.random.randn(pool_output_size, hidden_size)
        self.params['b2'] = np.zeros(hidden_size)
        self.params['W3'] = weight_init_std * \
                            np.random.randn(hidden_size, output_size)
        self.params['b3'] = np.zeros(output_size)

        # 계층 생성
        self.layers = OrderedDict()
        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],
                                           conv_param['stride'], conv_param['pad'])
        self.layers['Relu1'] = Relu()
        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)
        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])
        self.layers['Relu2'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])

        self.last_layer = SoftmaxWithLoss()

    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)

        return x

    def loss(self, x, t):
        """손실 함수를 구한다.

        Parameters
        ----------
        x : 입력 데이터
        t : 정답 레이블
        """
        y = self.predict(x)
        return self.last_layer.forward(y, t)

    def accuracy(self, x, t, batch_size=100):
        if t.ndim != 1 : t = np.argmax(t, axis=1)
        
        acc = 0.0
        
        for i in range(int(x.shape[0] / batch_size)):
            tx = x[i*batch_size:(i+1)*batch_size]
            tt = t[i*batch_size:(i+1)*batch_size]
            y = self.predict(tx)
            y = np.argmax(y, axis=1)
            acc += np.sum(y == tt) 
        
        return acc / x.shape[0]

    def numerical_gradient(self, x, t):
        """기울기를 구한다（수치미분）.

        Parameters
        ----------
        x : 입력 데이터
        t : 정답 레이블

        Returns
        -------
        각 층의 기울기를 담은 사전(dictionary) 변수
            grads['W1']、grads['W2']、... 각 층의 가중치
            grads['b1']、grads['b2']、... 각 층의 편향
        """
        loss_w = lambda w: self.loss(x, t)

        grads = {}
        for idx in (1, 2, 3):
            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])
            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])

        return grads

    def gradient(self, x, t):
        """기울기를 구한다(오차역전파법).

        Parameters
        ----------
        x : 입력 데이터
        t : 정답 레이블

        Returns
        -------
        각 층의 기울기를 담은 사전(dictionary) 변수
            grads['W1']、grads['W2']、... 각 층의 가중치
            grads['b1']、grads['b2']、... 각 층의 편향
        """
        # forward
        self.loss(x, t)

        # backward
        dout = 1
        dout = self.last_layer.backward(dout)

        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)

        # 결과 저장
        grads = {}
        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db
        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db

        return grads
        
    def save_params(self, file_name="params.pkl"):
        params = {}
        for key, val in self.params.items():
            params[key] = val
        with open(file_name, 'wb') as f:
            pickle.dump(params, f)

    def load_params(self, file_name="params.pkl"):
        with open(file_name, 'rb') as f:
            params = pickle.load(f)
        for key, val in params.items():
            self.params[key] = val

        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):
            self.layers[key].W = self.params['W' + str(i+1)]
            self.layers[key].b = self.params['b' + str(i+1)]
```

```python
# coding: utf-8
import sys, os
sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정
import numpy as np
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist
from simple_convnet import SimpleConvNet
from common.trainer import Trainer

# 데이터 읽기
(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)

# 시간이 오래 걸릴 경우 데이터를 줄인다.
#x_train, t_train = x_train[:5000], t_train[:5000]
#x_test, t_test = x_test[:1000], t_test[:1000]

max_epochs = 20

network = SimpleConvNet(input_dim=(1,28,28), 
                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},
                        hidden_size=100, output_size=10, weight_init_std=0.01)
                        
trainer = Trainer(network, x_train, t_train, x_test, t_test,
                  epochs=max_epochs, mini_batch_size=100,
                  optimizer='Adam', optimizer_param={'lr': 0.001},
                  evaluate_sample_num_per_epoch=1000)
trainer.train()

# 매개변수 보존
network.save_params("params.pkl")
print("Saved Network Parameters!")

# 그래프 그리기
markers = {'train': 'o', 'test': 's'}
x = np.arange(max_epochs)
plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)
plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()
```

## CNN 시각화하면
합성곱 계층을 여러 겹 쌓으면, **층이 깊어지면서 더 복잡하고 추상화된 정보가 추출**

**처음 층에서는 단순한 에지에 반응하고, 어이서 텍스처에 반응하고, 더 복잡한 사물의 일부에 반응하도록 변화함**

즉, 층이 깊어지면서 뉴런이 반응하는 대상이 단순한 모양에서 '고급' 정보로 변화해감, 다시 말하면 사물의 '의미'를 이해하도록 변화

## 대표적인 CNN
CNN 원조인 LeNet과 딥러닝이 주목받도록 이끈 AlexNet

### LeNet
![image](/assets/img/2025/lenet.jpeg)

합성곱 계층과 풀링 계층을 반복하고, 마지막으로 완전연결 계층을 거치면서 결과 출력

CNN과 비교하면

1. 활성화 함수(Lenet은 시그모이드 현재는 ReLU)
2. 원래 LeNet은 서브샘플링(데이터를 낮은 빈도로 샘플링했을 때의 샘플을 근사하는 연산)을 해서 중간 데이터 크기를 줄이지만 현재는 최대 풀링

### AlexNet
![image](/assets/img/2025/alexnet.jpg)

합성곱 계층과 풀링 계층을 거듭하여 완전연결 계층을 거쳐 결과를 출력

LeNet과 비교하면

1. 활성화함수로 ReLU
2. LRN(Local Response Normalization)이라는 국소적 정규화를 실시하는 계층 이용
3. 드롭아웃을 사용

# RNN(순환신경망: Recurrent Neural Network)
![image](/assets/img/2025/RNN.jpeg)

- **반복적이고 순차적인 데이터(Sequential data)학습에 특화된 인공신경망의 한 종류로써 내부의 순환구조가 들어있다는 특징**을 가지고 있음
- **순환구조를 이용하여 과거의 학습을 Weight를 통해 현재 학습에 반영, 기존의 지속적이고 반복적이며 순차적인 데이터 학습의 한계를 해결한 알고리즘**
- **현재의 학습과 과거의 학습을 연결 가능하게 하고 시간에도 종속된다는 특징**을 가짐
- 음성 웨이브폼을 파악하거나, 텍스트의 앞 뒤 성분을 파악할 때 주로 사용용