---
layout: post
title: backprop
description: > 
  backpropagation 오차역전파법
categories: [AI]
tags: [backpropagation]
---

[밑바닥부터 시작하는 딥러닝](https://www.yes24.com/Product/Goods/34970929)
BEYOND AI BASIC
{:.note title="출처 및 참고"}

* this unordered seed list will be replaced by the toc
{:toc}

신경망의 가중치 매개변수 기울기(가중치 매개변수에 대한 손실 함수의 기울기)는 수치 미분을 사용해 구현하였음

단점은 오래 걸린다는 것.

-> 오차역전파법

- 순전파(forward propagation): 왼쪽에서 오른쪽으로 진행
- 역전파(backward propagation): 오른쪽에서 왼쪽


역전파는 국소적인 미분을 순방향과는 다르게 반대방향으로

# 연쇄법칙
- 합성 함수의 미분에 대한 성질
> 합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.

x에 대한 미분은 t에 대한 z의 미분과 x에 대한 t의 미분의 곱으로 나타낼 수 있다.

$$
\frac{\partial z}{\partial x} = \frac{\partial z}{\partial t}\frac{\partial t}{\partial x}
$$

미분 결과

$$
\frac{\partial z}{\partial t} = 2t
$$

$$
\frac{\partial t}{\partial x} = 1
$$

최종적으로는 두 미분을 곱함
$$
\frac{\partial z}{\partial x} = \frac{\partial z}{\partial t}\frac{\partial t}{\partial x} = 2t * 1 = 2(x+y)
$$

- 역전파가 하는 일은 연쇄법칙의 원리와 같다

# 역전파
- 덧셈 노드의 역전파: 1을 곱하기만 함
```python
class MulLayer:
    def __init__(self):
        self.x = None
        self.y = None
    
    def forward(self, x, y):
        return x+y

    def backward(self, dout):
        dx = dout * 1
        dy = dout * 1
        return dx, dy
```


- 곱셈 노드의 역전파: 순전파 때의 입력 신호들을 서로 바꾼 값을 곱함

```python
class MulLayer:
    def __init__(self):
        self.x = None
        self.y = None
    
    def forward(self, x, y):
        self.x = x
        self.y = y
        out = x * y

        return out

    def backward(self, dout):
        dx = dout * self.y # x와 y를 바꾼다
        dy = dout * self.x

        return dx, dy
```

# 활성화 함수
## ReLU
- ReLU 수식
$$
\text{ReLU}(x) = 
\begin{cases}
0 & \text{if } x < 0 \\
x & \text{if } x \geq 0
\end{cases}
$$
- x에 대한 y의 미분
$$
\frac{\partial y}{\partial x} = 
\begin{cases}
0 & \text{if } x < 0 \\
x & \text{if } x \geq 0
\end{cases}
$$

```python
class Relu:
    def __init__(self):
        self.mask = None
    
    def forward(self, x):
        self.mask = (x <= 0)
        out = x.copy()
        out[self.mask] = 0
        return out

    def backward(self, dout):
        dout[self.mask] = 0
        dx = dout
        return dx
```

## Sigmoid
$$
y = \frac{1}{1+exp(-x)}
$$

- '/' 노드를 y=1/x를 미분하면 -y^2
- 'exp' 노드는 y=exp(x)를 미분하면 exp(x), 상류의 값에 순전파 떄의 출력(예시에서는 exp(-x))를 곱해 전파

$$
\frac{\partial L}{\partial y}y^2exp(-x) = 

\frac{\partial L}{\partial y}\frac{1}{(1+exp(-x))^2}exp(-x) \\
= \frac{\partial L}{\partial y}\frac{1}{1+exp(-x)\frac{exp(-x)}{1+exp(-x)}}\\
= \frac{\partial L}{\partial y}y(1-y)
$$

- sigmoid의 역전파는 순전파의 출력(y)만으로 계산이 가능해짐

```python
class Sigmoid:
    def __init__(self):
        self.out = None
    
    def forward(self, x):
        out 1 / (1 + np.exp(-x))
        self.out = out

        return out
    
    def backward(self, dout):
        dx = dout * (1.0 - self.out) * self.out

        return dx
```

# Affine/Softmax
## Affine 
순전파에서는 가중치 신호의 총합을 계산하기 때문에 행렬의 곱(np.dot)을 사용

> 신경망의 순전파 때 수행하는 행렬의 곱을 어파인 변환

np.dot(X, W) + B의 역전파를 전개하면
$$
\frac{\partial L}{\partial X}  = \frac{\partial L}{\partial Y}*W^T \\
\frac{\partial L}{\partial X} = X^T*\frac{\partial L}{\partial Y}
$$

데이터 N=2개인 경우 배치용 Affine
```python
class Affine:
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        self.dW = None
        self.db = None
    
    def forward(self, x):
        self.x = x
        out = np.dot(x, self.W) + self.b

        return out

    def backward(self, dout):
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0)

        return dx
```

## Softmax-with-Loss
- 출력층에서 사용하는 softmax, 입력 값을 정규화하여 출력
> 신경망 추론에서 답을 하나만 내는 경우에 가장 높은 점수만 알면 되므로 Sotmax는 필요가 없지만. 학습할 때는 필요

> 소프트맥스 함수의 손실 함수로 교차 엔트로피 오차를 사용하니 역전파가 (y_1 - t_1, y_2 - t_2, y_3 - t_3)으로 말끔하게 떨어짐. 이런건 우연이 아니라 교차 엔트로피 오차가 그렇게 설계 되어있기 때문

> 회귀의 출력층에서 사용하는 항등 함수의 손실오차로 오차제곱합을 사용하는 이유도 (y_1 - t_1, y_2 - t_2, y_3 - t_3)로 말끔하게 떨어지기 때문

```python
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None
        self.y = None
        self.t = None
    
    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy_error(self.x, self.t)
        return self.loss
    
    def backward(self, dout=1):
        batch_size = size.t.shape[0]
        dx = (self.y - self.t) / batch_size
        return dx
```

![image.png](/assets/img/2025/1.jpeg)